{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c9d2d0-e3a6-4c0e-8345-411203808c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.3.0-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached kafka_python-2.3.0-py2.py3-none-any.whl (326 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0ee358-4faf-4c11-a94f-85c5ed53c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-4.1.1-py2.py3-none-any.whl\n",
      "Collecting py4j<0.10.9.10,>=0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a58edeb-2c61-449a-9669-7a2c98dc5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b78ecd4-4db1-407b-bd80-d3baa303dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_2.13:{pyspark.__version__} pyspark-shell'\n",
    "os.environ['SPARK_SUBMIT_OPTS'] = '-Djdk.security.auth.login.Config=ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a746d83f-1ea4-475d-a551-0ddfb9443721",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"wikimedia_topic_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54256d5a-f907-4859-9014-9bb4d4050181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.31.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd222d04-56dd-41ac-95ae-c8b72dda2d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sseclient-py\n",
      "  Downloading sseclient_py-1.9.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading sseclient_py-1.9.0-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: sseclient-py\n",
      "Successfully installed sseclient-py-1.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sseclient-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46aa47a-3223-44e0-8e15-cbf68890d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca745cf9-8d1e-456b-8d5e-05deea82d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Note!!! **** At this point you need to have Kafka broker running. See Setup for Docker and Kafka.\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BROKER_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e082ea13-d21b-4967-b47c-7e83c3a60b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Wikimedia stream...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sseclient import SSEClient\n",
    "import threading\n",
    "\n",
    "# הגדרות (וודא ש-producer ו-KAFKA_TOPIC מוגדרים אצלך)\n",
    "URL = 'https://stream.wikimedia.org/v2/stream/recentchange'\n",
    "headers = {\n",
    "    \"User-Agent\": \"omer2\",\n",
    "    \"Authorization\": \"Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIxMDg0Yjg4NmZkMzFjYzViZmJlYTdjZTk2NGZjMmM3NiIsImp0aSI6IjJhYzdlOTdmNzQzN2ZlYzZiZDNiZjQ0OTM2OTI4YWFlZDE5ODU0Y2M3MzEyZTNhYmZmNGVmYmRlNzlhMGUwNGQ1NzM0ZjRhNTU4MDdkMGVhIiwiaWF0IjoxNzY4NDkzMzgzLjE3MzkxNiwibmJmIjoxNzY4NDkzMzgzLjE3MzkxOCwiZXhwIjozMzMyNTQwMjE4My4xNzAxMTYsInN1YiI6IjgxODc2Mjk4IiwiaXNzIjoiaHR0cHM6Ly9tZXRhLndpa2ltZWRpYS5vcmciLCJyYXRlbGltaXQiOnsicmVxdWVzdHNfcGVyX3VuaXQiOjUwMDAsInVuaXQiOiJIT1VSIn0sInNjb3BlcyI6WyJiYXNpYyJdfQ.eAMDwEBIDJY8OQIDVV_px_NbcDRpQO5Pu7M-UUmz_AnRy6UwebhNS3_9NhLbsT38q0ZEjbKymBTLQteDN2SfQUvbaiY2YEbKf26_JStoLpXw5anlPBySHo5S-0AlMfSwlhINneeuV8Ql_o1HAs2g-izUXOkTw_7kOvbP1p3DtVTKaqio3f7i5QEtDEhyBw4VC8-XHMbwL7pe8LXIR4vPF-GsVGIK4tD2ik7Qi_8XghEpx_D3sfay5cmjMrHcVGqvz3YuadlC7gXgE1Om5GfQZp95rFM7InFp1i4ohqOhFimVMfKhZ2_nXGechtQ1AnsKYQ9DBrtfjDu570jJIXnakEKkKumz-EW7JIQGMEIYkBnaQ2DobxtnAAinwlMXe7HcTQn3S6EZdmMF_KgtmHkGjR1NuZqJSW7eLBkp4KWWgZQ1nIJcqCvuiIYChu5VXFWcqCGubtJW8XKh1L8Sw-nUXIyNVvEe3JPXcjkDJsldRK1rahKpqf4DsRb28z10PVfoRc8lgi7YaV2h5PwFjdhhIBDYNRxejFMutbA27BfDiJJEvH7yoIE8HRU9zUv3jUpU6_iYwNgvhk74sDu5e_O7fuzwFcXydBt7Rn2fNTYbV-E1gFTfocxWA7B45yJjveyc4hNUhJ0uBaWd60pJbBUm_mL6HBC5Nwa3gmETOZGVBi8\" # הטוקן שהשגת \n",
    "}\n",
    "\n",
    "import requests\n",
    "from sseclient import SSEClient\n",
    "import threading\n",
    "\n",
    "def relay():\n",
    "    try:\n",
    "        response = requests.get(URL, headers=headers, stream=True, timeout=30)\n",
    "        events = SSEClient(response)\n",
    "        print(\"Connected to Wikimedia stream...\")\n",
    "\n",
    "        # שינוי כאן: הוספת .events() כדי לאפשר ריצה על האירועים\n",
    "        for event in events.events(): \n",
    "            if event.event == 'message' and event.data:\n",
    "                message = event.data.encode(\"utf-8\")\n",
    "                producer.send(KAFKA_TOPIC, value=message)\n",
    "                \n",
    "                # להדפסה לצורך בדיקה ראשונית בלבד:\n",
    "                # print(\"Message sent to Kafka\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in stream: {e}\")\n",
    "\n",
    "# הפעלת ה-Thread\n",
    "threading.Thread(target=relay).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "265ad257-2b0f-4083-aedc-6d4aa0c81d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-jupyter-streaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:{pyspark.__version__}\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"./checkpoint\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b86c37d-b1dd-4f00-adb4-08835651fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b6c1b5d-743b-4d1e-8613-582af40c9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"type\", StringType()) \\\n",
    "    .add(\"comment\", StringType()) \\\n",
    "    .add(\"user\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"server_name\", StringType())\n",
    "\n",
    "# Transform data to dataframe of json format\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89d8b737-8b44-4457-8f80-8ba5af47c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 14:51:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7d6a24101650>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+----------+--------------------+-----------------+--------------------+--------------------+\n",
      "|        id|      type|             comment|             user|               title|         server_name|\n",
      "+----------+----------+--------------------+-----------------+--------------------+--------------------+\n",
      "| 134485109|       log|Bot: Mass deletin...|        Ladsgroup|Thảo luận Thành v...|    vi.wikipedia.org|\n",
      "| 565757931|      edit|Annulation de la ...|           Morkoz|      Yassine Bounou|    fr.wikipedia.org|\n",
      "| 565757932|      edit|                    |   EricDuflot1968| Système de paiement|    fr.wikipedia.org|\n",
      "|1987174210|      edit|/* Beginnings (20...|    ~2026-38562-2|       Sam and Colby|    en.wikipedia.org|\n",
      "|  10802651|      edit|/* இளமை */ clean ...|S. ArunachalamBot|   பவித்ரா வெங்கடேஷ்|    ta.wikipedia.org|\n",
      "| 134485110|       log|Bot: Mass deletin...|        Ladsgroup|Thảo luận Thành v...|    vi.wikipedia.org|\n",
      "|      NULL|      edit|                 upd|         DeltaBot|User:DeltaBot/fix...|    www.wikidata.org|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|        Emijrpbot|File:Springpfuhlp...|commons.wikimedia...|\n",
      "| 134485111|       log|Bot: Mass deletin...|        Ladsgroup|Thảo luận Thành v...|    vi.wikipedia.org|\n",
      "|      NULL|      edit|Bot: Moving files...|        Sakib Bot|File:GNU Unifont ...|commons.wikimedia...|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|        Emijrpbot|File:Fengshan Tia...|commons.wikimedia...|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|Trollface 2006ALT|          Q137803703|    www.wikidata.org|\n",
      "|      NULL|categorize|[[:File:Могил бра...|       Rkieferbot|Category:Taken wi...|commons.wikimedia...|\n",
      "|      NULL|      edit|/* wbeditentity-s...|        Emijrpbot|File:Манеж у Ната...|commons.wikimedia...|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|        Emijrpbot|File:Nápoly (15).jpg|commons.wikimedia...|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|            Rebot|          Q107133588|    www.wikidata.org|\n",
      "|      NULL|      edit|/* wbsetclaim-cre...|        Emijrpbot|File:Örebro på cy...|commons.wikimedia...|\n",
      "|     85291|       new|            Welcome!|      Welcome-Bot|ကေားသုင်ꩻသား အိုင...|   blk.wikipedia.org|\n",
      "| 565757933|      edit|Suppression des p...|        LocpacBot|        Juifs arabes|    fr.wikipedia.org|\n",
      "|      NULL|       new|[[Special:MyLangu...|            NickK|Category:2011-11-...|commons.wikimedia...|\n",
      "+----------+----------+--------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac1fa73-e13b-44a3-9046-e8dfd6356777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 14:51:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7d6a0d0201d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 14:51:20 WARN MicroBatchExecution: Disabling AQE since AQE is not supported in stateful workloads.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|                user|count|\n",
      "+--------------------+-----+\n",
      "| Ogidzatul Azis Sueb|    1|\n",
      "|       La Graillance|    3|\n",
      "|Петроченко Віктор...|    1|\n",
      "|            Sukkoria|   13|\n",
      "|       ~2026-38402-8|    1|\n",
      "|            Fuwuyuan|    3|\n",
      "|              Equord|    1|\n",
      "|         JeiAllenYes|    9|\n",
      "|             EGA0250|    1|\n",
      "|        Sneeuwvlakte|  102|\n",
      "|Bernd Schwabe in ...|   15|\n",
      "|               M0tty|    3|\n",
      "|         Rojiblancos|    1|\n",
      "|       DenisMironov1|    8|\n",
      "|            MTheiler|    1|\n",
      "|           LocpacBot|   31|\n",
      "|           B.mertlik|    2|\n",
      "|           Trident90|    1|\n",
      "|    AnarchistiCookie|    3|\n",
      "|         Bogdan 1956|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "parsed_df.createOrReplaceTempView(\"parsed_df\")\n",
    "\n",
    "spark.sql(\"select user, count(*) as count from parsed_df group by user\") \\\n",
    ".writeStream \\\n",
    ".outputMode(\"complete\") \\\n",
    ".format(\"console\") \\\n",
    ".start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0662a560-afa8-49cc-b7dd-0957df44aa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 14:51:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7d6a0d023210>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 14:51:34 WARN MicroBatchExecution: Disabling AQE since AQE is not supported in stateful workloads.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+-----+\n",
      "|      type|count|\n",
      "+----------+-----+\n",
      "|       new|  240|\n",
      "|       log| 1094|\n",
      "|      edit| 4327|\n",
      "|categorize| 5537|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select type, count(*) as count from parsed_df group by type\") \\\n",
    ".writeStream \\\n",
    ".outputMode(\"complete\") \\\n",
    ".format(\"console\") \\\n",
    ".start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edec8ba-2d14-4513-a4c5-8561820adf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
